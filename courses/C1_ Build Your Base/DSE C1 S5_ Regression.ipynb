{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C-HACK Tutorial 5: Regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python (tunnel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZhMpsTQGPQIm"},"source":["# **C-HACK Tutorial 5: Data Regression Analysis**"]},{"cell_type":"markdown","metadata":{"id":"M-4Y8PRWQBwW"},"source":["**Instructor**: Stephanie Valleau <br>\n","**Contact**: valleau@uw.edu"]},{"cell_type":"markdown","metadata":{"id":"5jFqEBrlgoa1"},"source":["**Acknowlegments:** Stephanie would like acknowledge the entire C-Hack hackmaster team for review and comments on the tutorial. In particular, this tutorial was reviewed by:\n","\n","* Kenny Andre\n","* Jordy Jackson\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LXXn9xy52KPj"},"source":["In this tutorial we will look at fitting data to a curve using **regression**. We will also look at using regression to make **predictions** for new data points by dividing our data into a training and a testing set. Finally we will examine how much error we make in our fit and then in our predictions by computing the mean squared error, at the variance and bias."]},{"cell_type":"markdown","metadata":{"id":"F2fOL1IBhB-o"},"source":["\n","\n","---\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jc0SbuFeQBwW"},"source":["### Load libraries which will be needed in this Notebook\n","\n"]},{"cell_type":"code","metadata":{"id":"XA8E1GTQQBwW","executionInfo":{"status":"ok","timestamp":1610499988698,"user_tz":480,"elapsed":1971,"user":{"displayName":"Stephanie Valleau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64","userId":"13099634571785749992"}}},"source":["# Pandas library for the pandas dataframes\n","import pandas as pd    \n","\n","# Import Scikit-Learn library for the regression models\n","import sklearn         \n","from sklearn import linear_model, datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Import numpy \n","import numpy as np\n","\n","# Import plotting libraries\n","import seaborn as sns\n","import matplotlib \n","from matplotlib import pyplot as plt\n","\n","# Set larger fontsize for all plots\n","matplotlib.rcParams.update({'font.size': 20})\n","\n","# Command to automatically reload modules before executing cells\n","# not needed here but might be if you are writing your own library \n","%load_ext autoreload\n","%autoreload 2\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rDyhsVetrbhX"},"source":["### Load our google drive folder & find path of files"]},{"cell_type":"markdown","metadata":{"id":"f-Y1TU5froPq"},"source":["When you execute the cell below you will be asked to give authorization to colab to access your drive. "]},{"cell_type":"code","metadata":{"id":"sdr7-pi9rYrV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610500012802,"user_tz":480,"elapsed":26066,"user":{"displayName":"Stephanie Valleau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64","userId":"13099634571785749992"}},"outputId":"2ff73e21-b5d9-4985-84b5-23ad7b5918cd"},"source":["# Sync your google drive folder\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wS-VSJYQr_7a"},"source":["Follow the instructions shown in *Evan Komp's* [video](https://youtu.be/5rZn-aVNR0A) to locate the file called \"running_cheetah.csv\" and save its path in a variable"]},{"cell_type":"code","metadata":{"id":"WkGCOHHlsEoo","executionInfo":{"status":"ok","timestamp":1610500106137,"user_tz":480,"elapsed":621,"user":{"displayName":"Stephanie Valleau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64","userId":"13099634571785749992"}}},"source":["your_path_to_cheetah_file=\"/content/drive/MyDrive/C-HACK 2021 EVENT/Tutorials/Tutorial 5/running_cheetah.csv\""],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cm6isl1eQBwX"},"source":["## 5.1 What is regression? "]},{"cell_type":"markdown","metadata":{"id":"crBj3b4FHemQ"},"source":["It is the process of finding a relationship between **_dependent_** and **_independent_** variables to find trends in data. This abstract definition means that you have one variable (the dependent variable) which depends on one or more variables (the independent variables). One of the reasons for which we want to regress data is to understand whether there is a trend between two variables. For instance, let's say we had data points showing how tired we are respect to the amount of coffee we drink. In the case of linear regression, the question would be - is the data quantity of how tired I am linearly proportional to the amount of coffee I drink, i.e. can we use a line to represent the relationship between these two variables?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qppNLSFvQBwX"},"source":["## 5.2  Linear regression + a Cheetah: fitting with scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"eOjPYw3SQBwX"},"source":["### 5.2.1 Loading the dataset"]},{"cell_type":"markdown","metadata":{"id":"MAE_OOndQBwX"},"source":["Consider a Cheetah on the run (worth watching): https://www.youtube.com/watch?v=8-9oFxYFODE\n","<br>\n"]},{"cell_type":"markdown","metadata":{"id":"YFPA80XH1qeu"},"source":["![Image of Cheetah](https://drive.google.com/uc?export=view&id=1lpZinFnWbbreZxx2BxpldWPVBlAyUL89)"]},{"cell_type":"markdown","metadata":{"id":"z0r6rBAhQBwX"},"source":["Assume you have a set of data with values (time, position, speed, energy) for one Cheetah's motion. The dataset is in csv format, lets load it by using the <code> Pandas </code> library - the name of the file is <code> running_cheetah.csv </code>\n","\n","\n","\n","```\n","# Load the dataset using the read_csv() pandas function - we have to indicate that\n","# the index of each row is in the first column\n","cheetah_df=pd.read_csv(\"running_cheetah.csv\",index_col=0)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"llseELesQBwY","executionInfo":{"status":"ok","timestamp":1610500113205,"user_tz":480,"elapsed":2264,"user":{"displayName":"Stephanie Valleau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxYC_xbIb6qmkXo8iMld6AvpYL5dO54osIaSx=s64","userId":"13099634571785749992"}}},"source":["# Load the dataset using the read_csv() pandas function - we have to indicate that\n","# the index of each row is in the first column\n","cheetah_df=pd.read_csv(your_path_to_cheetah_file,index_col=0)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAcHDd1vQBwY"},"source":["#### Exercise 1\n","What does the data look like? Remember how to visualize data in a pandas dataframe (Tutorial 2) \n"]},{"cell_type":"code","metadata":{"id":"0-TMjSfuQBwY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9vavdktFQBwZ"},"source":["### 5.2.2 Visualizing the data set"]},{"cell_type":"markdown","metadata":{"id":"wy3XoXStQBwZ"},"source":["We can create a scatter plot of positions of the Cheetah as a function of time using the <code> plt.scatter() </code> function (as discussed in Tutorial 4)"]},{"cell_type":"markdown","metadata":{"id":"qAvooLBfQBwZ"},"source":["#### Exercise 2 \n","Create a scatter plot of positions of the Cheetah respect to time\n"]},{"cell_type":"code","metadata":{"id":"36JsJFWNQBwZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gp_Ssxc1QBwZ"},"source":["### 5.2.3 Linear regression - fitting data to a line\n","\n","It looks like position keeps increasing in time following a line, maybe something like\n","\n","$$y(t)= m\\cdot t + b  \\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 1}$$ \n","\n","with $y=\\sf position$, $t=\\sf time$, $m$ the slope and $b$ the intercept. \n","\n","To solve the problem, we need to find the values of $b$ and $m$ in equation 1 to best fit the data. This is called **linear regression**.\n","\n","In linear regression our goal is to minimize the error between computed values of positions $y^{\\sf calc}(t_i)\\equiv y^{\\sf calc}_i$ and known values $y^{\\sf exact}(t_i)\\equiv y^{\\sf exact}_i$, i.e. find $b$ and $m$ which lead to lowest value of\n","\n","$$\\epsilon (m,b) =\\sum_{i=1}^{N_{\\sf time points}}\\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2 = \\sum_{i=1}^{N_{\\sf time points}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\sf{eq. 2}$$\n","\n","To find out more see e.g. https://en.wikipedia.org/wiki/Simple_linear_regression"]},{"cell_type":"markdown","metadata":{"id":"18lLAfaHQBwZ"},"source":["#### Question 1\n","Do we always want *m* and *b* to be large positive numbers so as to minimize eq. 2? Respond to the **polleverywhere** quiz - https://pollev.com/weloveds "]},{"cell_type":"markdown","metadata":{"id":"Gc60p76jQBwZ"},"source":["### 5.2.4 The scikit-learn library\n","\n","Luckily there is a Python library called [scikit-learn](https://scikit-learn.org/stable/) which contains many functions related to regression including [linear regression](https://scikit-learn.org/stable/modules/linear_model.html). \n","\n","The function we will use is called <code> LinearRegression() </code>. "]},{"cell_type":"markdown","metadata":{"id":"cxPnpy8jQBwZ"},"source":["### 5.2.5 Fitting the Cheetah position in time data to a line\n","\n","```\n","# Create linear regression object\n","regr = linear_model.LinearRegression()\n","\n","# Use model to fit to the data, the X values are times and the Y values are positions of the Cheetah\n","# Note that we need to reshape the vectors to be of the shape X - (n_samples, n_features) and Y (n_samples, n_targets)\n","X = cheetah_df['time [hr]'].values.reshape(-1, 1)\n","Y = cheetah_df['position [miles]'].values.reshape(-1, 1)\n","```"]},{"cell_type":"code","metadata":{"id":"AUWdaOgMQBwZ"},"source":["# Create linear regression object\n","regr = linear_model.LinearRegression()\n","\n","# Use model to fit to the data, the X values are times and the Y values are positions of the Cheetah\n","# Note that we need to reshape the vectors to be of the shape X - (n_samples, n_features) and Y (n_samples, n_targets)\n","X = cheetah_df['time [hr]'].values.reshape(-1, 1)\n","Y = cheetah_df['position [miles]'].values.reshape(-1, 1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M7dszV5EA-hL"},"source":["```\n","print(cheetah_df['time [hr]'].values.shape, cheetah_df['position [miles]'].values.shape)\n","print(X.shape, Y.shape)\n","```"]},{"cell_type":"code","metadata":{"id":"d25DN-kRAzw8"},"source":["print(cheetah_df['time [hr]'].values.shape, cheetah_df['position [miles]'].values.shape)\n","print(X.shape, Y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wD7EekCNBDiB"},"source":["```\n","# Fit to the data\n","regr.fit(X, Y)\n","\n","# Extract the values of interest\n","m = regr.coef_[0][0]\n","b = regr.intercept_[0]\n","\n","# Print the slope m and intercept b\n","print('Scikit learn - Slope: ', m , 'Intercept: ', b )\n","```"]},{"cell_type":"code","metadata":{"id":"eFH0SXRkAnDB"},"source":["# Fit to the data\n","regr.fit(X, Y)\n","\n","# Extract the values of interest\n","m = regr.coef_[0][0]\n","b = regr.intercept_[0]\n","\n","# Print the slope m and intercept b\n","print('Scikit learn - Slope: ', m , 'Intercept: ', b )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4uFUK6QfQBwa"},"source":["#### Exercise 3\n","Estimate the values of $y$ by using your fitted parameters. Hint: Use your <code>regr.coef_</code> and <code>regr.intercept_</code> parameters to estimate Y_calc_2 following equation 1\n"]},{"cell_type":"code","metadata":{"id":"VFVrL1VQQBwa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DFlw26PQBwa"},"source":["# Another way to get this is using the regr.predict function\n","Y_calc = regr.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVoA-OG4QBwa"},"source":["Let's create a graphical visualization of the fitted values respect to the exact values\n","\n","```\n","# Lets plot exact positions respect to the time values using a scatter plot\n","plt.scatter(cheetah_df['time [hr]'], cheetah_df['position [miles]'], s=100, marker='.', color=\"cornflowerblue\")\n","plt.xlabel('time [hours]')\n","plt.ylabel('position [miles]')\n","\n","# Now we compare to our fit by plotting both Y_cal and Y_calc_2 respect to time \n","plt.plot(X, Y_calc, color='crimson',linewidth=3, label='linear fit')\n","plt.plot(X, Y_calc_2,':', color='black',linewidth=3, label='linear fit check')\n","plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n","plt.show()\n","```"]},{"cell_type":"code","metadata":{"id":"cI6s967TQBwa"},"source":["# Lets plot exact positions respect to the time values using a scatter plot\n","plt.scatter(cheetah_df['time [hr]'], cheetah_df['position [miles]'], s=100, marker='.', color=\"cornflowerblue\")\n","plt.xlabel('time [hours]')\n","plt.ylabel('position [miles]')\n","\n","# Now we compare to our fit by plotting both Y_cal and Y_calc_2 respect to time \n","plt.plot(X, Y_calc, color='crimson',linewidth=3, label='linear fit')\n","plt.plot(X, Y_calc_2,':', color='black',linewidth=3, label='linear fit check')\n","plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FSsBFNvcQBwb"},"source":["## 5.3 How much error are we making? "]},{"cell_type":"markdown","metadata":{"id":"KKfamij9QBwb"},"source":["### 5.3.1 Mean Squared Error\n","\n","The plot in Section 5.2.5 looks good, but numerically what is our error? What is the mean value of $\\epsilon$, i.e. the **Mean Squared Error (MSE)**?\n","\n","$${\\sf MSE}=\\epsilon_{\\sf ave} = \\frac{\\sum_{i=1}^{N_{\\sf times}}\\left(y^{\\sf exact}_i - m\\cdot t_i - b \\right)^2}{N_{\\sf times}}\\;\\;\\;\\;\\;\\sf eq. 3$$\n","\n","```\n","# The mean squared error\n","print('Mean squared error: %.2f' % mean_squared_error(Y, Y_calc))\n","```"]},{"cell_type":"code","metadata":{"id":"BDCUEKNQQBwb"},"source":["# The mean squared error\n","print('Mean squared error: %.2f' % mean_squared_error(Y, Y_calc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5m88vMlkQBwb"},"source":["Another way to measure error is the regression score, $R^2$. $R^2$ is generally defined as the ratio of the total sum of squares $SS_{\\sf tot} $ to the residual sum of squares $SS_{\\sf res} $:\n","\n","$$SS_{\\sf tot}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i-\\bar{y}\\right)^2\\;\\;\\;\\;\\; \\sf eq. 4$$\n","$$SS_{\\sf res}=\\sum_{i=1}^{N} \\left(y^{\\sf exact}_i - y^{\\sf calc}_i\\right)^2\\;\\;\\;\\;\\; \\sf eq. 5$$\n","$$R^2 = 1 - {SS_{\\sf res}\\over SS_{\\sf tot}} \\;\\;\\;\\;\\;\\; \\sf eq. 6$$\n","\n","In eq. 4, $\\bar{y}=\\sum_i y^{\\sf exact}_i/N$ is the average value of y for $N$ points. The best value of $R^2$ is 1 but it can also take a negative value if the error is large.\n","\n","See all the different regression metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html)."]},{"cell_type":"markdown","metadata":{"id":"L5qg7H3_QBwb"},"source":["#### Question 3\n","Do we need a large value of $SS_{\\sf tot}$ to minimize $R^2$ - is this something which we have the power to control? Respond to the **polleverywhere** quiz - https://pollev.com/weloveds "]},{"cell_type":"markdown","metadata":{"id":"7maWcZuQ_CVD"},"source":["```\n","# Print the coefficient of determination - 1 is perfect prediction\n","print('Coefficient of determination: %.2f' % r2_score(Y, Y_calc))\n","```"]},{"cell_type":"code","metadata":{"id":"Ejt0nyHJQBwb"},"source":["# Print the coefficient of determination - 1 is perfect prediction\n","print('Coefficient of determination: %.2f' % r2_score(Y, Y_calc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ApcE4sHhQBwb"},"source":["### 5.3.2 Predicting with a fitted model - Bias and Variance"]},{"cell_type":"markdown","metadata":{"id":"q0fkdCeBQBwb"},"source":["We could have used a part of the data to fit our line - **_training_** data - and the rest to validate it - **_test_** data. We will see an example of this in Section 5.4. At that point, the <code> predict() </code> function would have predicted previoulsy unseen positions. In the case when a subset of training data is used to find the parameters of the model we refer to the model as *_trained_* once the optimal values of the parameters have been determined.\n","\n","Here, we are interested in evaluating the accuracy of the trained model when it predicts data from the test dataset. The error can be divided in to a **_irreducible_** and **_reducible_** part. \n","\n","The **_irreducible error_** comes from the fact that data is noisy.\n","\n","\n","\n","For more information see [here](https://www.coursera.org/lecture/ml-regression/irreducible-error-and-bias-qlMrZ). \n","\n","The **_reducible error_** of the model , i.e. the error which comes from the mismatch of the model function $y=m_{\\sf opt}x+b_{\\sf opt}$ respect to the exact data $\\left\\{y_i\\right\\}_{i=1}^N$ is then broken down into **_bias_** and **_variance_**. To understand what these two terms mean we will consider the problem of the Cheetah. \n"]},{"cell_type":"markdown","metadata":{"id":"qs2K_vbzQBwb"},"source":["### 5.3.2.1 Variance\n","\n","Let's go back to our Cheetah problem and train the model on three different sets of training points\n","\n","```\n","# Define three datasets from the original set by selecting a range of values using the np.argwhere() function\n","\n","# Search for positions in each numpy array where the values are in a specific range of time e.g. time < 35 hr\n","indices_test = [ind[0] for ind in np.argwhere(X <= 35)]\n","indices1 = [ind[0] for ind in np.argwhere((X > 35) & (X < 50))]\n","indices2 = [ind[0] for ind in np.argwhere((X >= 50) & (X < 70))]\n","indices3 = [ind[0] for ind in np.argwhere(X >= 70)]\n","\n","# Define the three training dataset and testing datasets by using the indices \n","X1_train = X[indices1] \n","Y1_train = Y[indices1] \n","\n","X2_train = X[indices2]\n","Y2_train = Y[indices2]\n","\n","X3_train = X[indices3]\n","Y3_train = Y[indices3]\n","\n","X_test = X[indices_test]\n","Y_test = Y[indices_test]\n","```"]},{"cell_type":"code","metadata":{"id":"ib5yfvvTQBwb"},"source":["# Define three datasets from the original set by selecting a range of values using the np.argwhere() function\n","\n","# Search for positions in each numpy array where the values are in a specific range of time e.g. time < 35 hr\n","indices_test = [ind[0] for ind in np.argwhere(X <= 35)]\n","indices1 = [ind[0] for ind in np.argwhere((X > 35) & (X < 50))]\n","indices2 = [ind[0] for ind in np.argwhere((X >= 50) & (X < 70))]\n","indices3 = [ind[0] for ind in np.argwhere(X >= 70)]\n","\n","# Define the three training dataset and testing datasets by using the indices \n","X1_train = X[indices1] \n","Y1_train = Y[indices1] \n","\n","X2_train = X[indices2]\n","Y2_train = Y[indices2]\n","\n","X3_train = X[indices3]\n","Y3_train = Y[indices3]\n","\n","X_test = X[indices_test]\n","Y_test = Y[indices_test]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RK3NWCxM_QxK"},"source":["```\n","# Let's plot the three training sets and the test set\n","plt.scatter(X1_train, Y1_train, s=100, marker='.', color=\"royalblue\", label=\"training set 1\")\n","plt.scatter(X2_train, Y2_train, s=100, marker='.', color=\"indianred\", label=\"training set 2\")\n","plt.scatter(X3_train, Y3_train, s=100, marker='.', color=\"mediumseagreen\", label=\"training set 3\")\n","plt.scatter(X_test, Y_test, s=100, marker='.', color=\"grey\", label=\"test set 3\")\n","plt.xlabel('time [hours]')\n","plt.ylabel('position [miles]')\n","plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n","plt.show()\n","```"]},{"cell_type":"code","metadata":{"id":"ngR9-UpO_JHR"},"source":["# Let's plot the three training sets and the test set\n","plt.scatter(X1_train, Y1_train, s=100, marker='.', color=\"royalblue\", label=\"training set 1\")\n","plt.scatter(X2_train, Y2_train, s=100, marker='.', color=\"indianred\", label=\"training set 2\")\n","plt.scatter(X3_train, Y3_train, s=100, marker='.', color=\"mediumseagreen\", label=\"training set 3\")\n","plt.scatter(X_test, Y_test, s=100, marker='.', color=\"grey\", label=\"test set 3\")\n","plt.xlabel('time [hours]')\n","plt.ylabel('position [miles]')\n","plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qL3-8SU4QBwb"},"source":["Now we will proceed just like we did in Section 5.2.5 and carry out linear regression on each of these three training sets\n","\n","```\n","regr1 = linear_model.LinearRegression()\n","regr1.fit(X1_train, Y1_train)\n","\n","regr2 = linear_model.LinearRegression()\n","regr2.fit(X2_train, Y2_train)\n","\n","regr3 = linear_model.LinearRegression()\n","regr3.fit(X3_train, Y3_train)\n","\n","m1 = regr1.coef_[0][0]\n","b1 = regr1.intercept_[0]\n","\n","m2 = regr2.coef_[0][0]\n","b2 = regr2.intercept_[0]\n","\n","m3 = regr3.coef_[0][0]\n","b3 = regr3.intercept_[0]\n","```"]},{"cell_type":"code","metadata":{"id":"dxpHTvcKQBwb"},"source":["regr1 = linear_model.LinearRegression()\n","regr1.fit(X1_train, Y1_train)\n","\n","regr2 = linear_model.LinearRegression()\n","regr2.fit(X2_train, Y2_train)\n","\n","regr3 = linear_model.LinearRegression()\n","regr3.fit(X3_train, Y3_train)\n","\n","m1 = regr1.coef_[0][0]\n","b1 = regr1.intercept_[0]\n","\n","m2 = regr2.coef_[0][0]\n","b2 = regr2.intercept_[0]\n","\n","m3 = regr3.coef_[0][0]\n","b3 = regr3.intercept_[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LT36gUdN_aUr"},"source":["```\n","# Let's see how well they fit/predict the total dataset (note that usually we would only look at the test set here)\n","Y_calc_1 = regr1.predict(X_test)\n","Y_calc_2 = regr2.predict(X_test)\n","Y_calc_3 = regr3.predict(X_test)\n","```"]},{"cell_type":"code","metadata":{"id":"kaTICfwCQBwb"},"source":["# Let's see how well they fit/predict the total dataset (note that usually we would only look at the test set here)\n","Y_calc_1 = regr1.predict(X_test)\n","Y_calc_2 = regr2.predict(X_test)\n","Y_calc_3 = regr3.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lo85wEPOQBwb"},"source":["Let's visualize the fitted values using our trained models and compare to the average fit\n","\n","```\n","# Lets plot exact positions respect to the time values using a scatter plot\n","plt.scatter(X_test, Y_test, s=100, marker='o', facecolors='lightgrey', edgecolors='slategrey', label=\"test data\")\n","plt.xlabel('time [hours]')\n","plt.ylabel('position [miles]')\n","\n","# Compute the average calculated position\n","Y_calc_ave =  ( Y_calc_1 + Y_calc_2 + Y_calc_3 ) / 3\n","\n","# Now we compare to our fit by plotting both Y_cal and Y_calc_2 respect to time \n","plt.plot(X_test, Y_calc_1, color='royalblue',linewidth=3, label='linear fit 1')\n","plt.plot(X_test, Y_calc_2, color='indianred',linewidth=3, label='linear fit 2')\n","plt.plot(X_test, Y_calc_3, color='mediumseagreen',linewidth=3, label='linear fit 3')\n","plt.plot(X_test, Y_calc_ave, color='dimgrey',linewidth=3, label='average fit')\n","plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n","plt.show()\n","```"]},{"cell_type":"code","metadata":{"id":"JZCSJGtjQBwb"},"source":["# Lets plot exact positions respect to the time values using a scatter plot\n","plt.scatter(X_test, Y_test, s=100, marker='o', facecolors='lightgrey', edgecolors='slategrey', label=\"test data\")\n","plt.xlabel('time [hours]')\n","plt.ylabel('position [miles]')\n","\n","# Compute the average calculated position\n","Y_calc_ave =  ( Y_calc_1 + Y_calc_2 + Y_calc_3 ) / 3\n","\n","# Now we compare to our fit by plotting both Y_cal and Y_calc_2 respect to time \n","plt.plot(X_test, Y_calc_1, color='royalblue',linewidth=3, label='linear fit 1')\n","plt.plot(X_test, Y_calc_2, color='indianred',linewidth=3, label='linear fit 2')\n","plt.plot(X_test, Y_calc_3, color='mediumseagreen',linewidth=3, label='linear fit 3')\n","plt.plot(X_test, Y_calc_ave, color='dimgrey',linewidth=3, label='average fit')\n","plt.legend(bbox_to_anchor=(1.8, 1), loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S9i5tuSnQBwc"},"source":["#### Question 4\n","Why do you think each linear fit has a different slope and intercept? Respond to the **polleverywhere** quiz - https://pollev.com/weloveds \n"]},{"cell_type":"markdown","metadata":{"id":"-m8u65UjQBwc"},"source":["In the plot we see that each trained model has a different slope and intercept. The average fit is shown in grey. The difference between what one of the trained model predicts and the average model prediction is what we call the **_variance_**.\n","\n","The variance is the amount by which the prediction will change if different subsets of the training data sets are used. We generally want to have a low variance as that ensure that our model is not sensitive to small fluctuations in the dataset.\n","\n","Large variance occurs when the model performs well on the training dataset but does not do well on the test dataset.\n","\n","If we use more complex models to fit, for example polynomials, that will generally lead to a larger variance."]},{"cell_type":"markdown","metadata":{"id":"EpNiMOObQBwc"},"source":["### 5.3.2.2 Bias"]},{"cell_type":"markdown","metadata":{"id":"BOnHzKWFQBwc"},"source":["Now, let's imagine that we are given the true universal Cheetah position function, lets call it <code> true_cheetah </code> we define it below\n","\n","```\n","def true_cheetah(t):\n","    return 63.64 * t +  155.03\n","```"]},{"cell_type":"code","metadata":{"id":"YAhhvCJcQBwc"},"source":["def true_cheetah(t):\n","    return 63.64 * t +  155.03"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54BMMnMOQBwc"},"source":["How well are we doing in terms of our prediction error respect to the true values? Are we biased? Let's visualize our results\n","\n","```\n","plt.plot(X_test, true_cheetah(X_test), color='mediumseagreen',linewidth=3, label='true cheetah motion')\n","plt.plot(X_test, Y_calc_ave, color='dimgrey',linewidth=3, label='average fit')\n","plt.scatter(X_test, Y_test, s=100, marker='o', facecolors='lightgrey', edgecolors='slategrey', label=\"test data\")\n","plt.xlabel('time [hours]')\n","plt.ylabel('position [miles]')\n","plt.legend(bbox_to_anchor=(1.9, 1), loc='upper right')\n","plt.show()\n","```"]},{"cell_type":"code","metadata":{"id":"RHhbr1THQBwc"},"source":["plt.plot(X_test, true_cheetah(X_test), color='mediumseagreen',linewidth=3, label='true cheetah motion')\n","plt.plot(X_test, Y_calc_ave, color='dimgrey',linewidth=3, label='average fit')\n","plt.scatter(X_test, Y_test, s=100, marker='o', facecolors='lightgrey', edgecolors='slategrey', label=\"test data\")\n","plt.xlabel('time [hours]')\n","plt.ylabel('position [miles]')\n","plt.legend(bbox_to_anchor=(1.9, 1), loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggJ6fJqOQBwc"},"source":["We see that our model is biased, indeed our average fit model predicts with a **_bias_** error respect to the true cheetah motion. \n","\n","In this case, a more complex function, such as a polynomial might give a better fit and a lower **_bias_**.\n","<div>\n","\n","<img src=\"attachment:cheetah_over_underfitting.png\" align=left width=\"400\"/>\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"Y1vW-7Es4ISD"},"source":["\n","<div>\n","<img src=https://drive.google.com/uc?export=view&id=1DXLgVKjzBrofSPtxtNcVb9Lf_FbUMcKe width=\"500\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"Lj5_0CMZQBwc"},"source":["We usually want to find a sweet spot which minimizes both bias and variance. As we increase complexity, for instance by using a polynomial fit, our bias goes down however variance increases. Similarly for a very simple model the bias is large while the variance is small. We can see this graphically in the sketch copied below\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9pAxYLVq5yHN"},"source":["<div>\n","<img src=https://drive.google.com/uc?export=view&id=1phnDnjjGJ8twf7h9JKS3TbbN5QlBFj9o width=\"500\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"bbXhulIfQBwc"},"source":["### 5.3.3 Beyond a single input feature"]},{"cell_type":"markdown","metadata":{"id":"tizRPJ_GQBwc"},"source":["The **speed** or velocity of the Cheetah (the dependent variable v) could depend on:\n","* his/her **energy** (independent variable E) that day\n","* how well the Cheetah **slept** the night before (independent variable sleep, S) \n","* whether she/he was well **fed** (independent variable, F) \n","* how much he or she has been **training** recently (independent variable, T)\n","\n","You will look at using multiple features in the breakout room\n","<hr style=\"border:1px solid grey\"> </hr>\n"]},{"cell_type":"markdown","metadata":{"id":"UmZ5TyuBQBwc"},"source":["## 5.4 Many independent variables - Boston Housing data"]},{"cell_type":"markdown","metadata":{"id":"2_px547fUNsA"},"source":["What does it contain? You saw it in Tutorials 3 and 4! Let's load it again. For more info - https://scikit-learn.org/stable/datasets/index.html#boston-dataset"]},{"cell_type":"markdown","metadata":{"id":"8CON6vQDQBwc"},"source":["### 5.4.1 Load the Boston data into a dictionary a dataframe and X and y numpy arrays\n","\n","```\n","# Load the dataset into a dictionary and into a dataframe\n","boston_dict = datasets.load_boston() # A dictionary object \n","boston_df = pd.DataFrame(boston_dict.data, columns=boston_dict.feature_names) # A Pandas dataframe\n","boston_df['MEDV'] = boston_dict.target # Define the dependent variable as the housing cost 'MEDV'\n","```"]},{"cell_type":"code","metadata":{"id":"trZCxDxRFnH1"},"source":["# Load the dataset into a dictionary and into a dataframe\n","boston_dict = datasets.load_boston() # A dictionary object \n","boston_df = pd.DataFrame(boston_dict.data, columns=boston_dict.feature_names) # A Pandas dataframe\n","boston_df['MEDV'] = boston_dict.target # Define the dependent variable as the housing cost 'MEDV'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O0oDva0xQBwc"},"source":["#### Exercise 5\n","Check that the dataframe looks good by printing out it's content\n"]},{"cell_type":"code","metadata":{"id":"crki66jZQBwc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FqYrXk4o_1aL"},"source":["```\n","# X will contain the independent variables and y the dependent variable\n","X, y = datasets.load_boston(return_X_y=True) # To be used when we want to carry out regression\n","```"]},{"cell_type":"code","metadata":{"id":"1jtT3txAQBwc"},"source":["# X will contain the independent variables and y the dependent variable\n","X, y = datasets.load_boston(return_X_y=True) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9BfuJFk4QBwc"},"source":["### 5.4.2 Visualizing the housing prices - the dependent variable, or \"output label\"\n","\n","The value we aim to predict or evaluate is the price of housing. This is our dependent variable. Respect to the case of the Cheetah, we will look at how it is related to the 13 independent variables, also known as *input features*. These variables were listed a few cells above and inclue i.e. CRIM, TAX etc..."]},{"cell_type":"markdown","metadata":{"id":"Lj-4l1x6_5RS"},"source":["```\n","# We can make a histogram as seen in Tutorial 4\n","boston_df['MEDV'].plot(kind='hist', bins=30, color=\"rebeccapurple\", alpha=0.8)\n","plt.xlim([0,60])\n","plt.show()\n","\n","```"]},{"cell_type":"code","metadata":{"id":"Qw_MvwxmQBwc"},"source":["# We can make a histogram as seen in Tutorial 4\n","boston_df['MEDV'].plot(kind='hist', bins=30, color=\"rebeccapurple\", alpha=0.8)\n","plt.xlim([0,60])\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p3ZpllJdQBwc"},"source":["### 5.4.3 Recall from Tutorial 4 the 13 independent variables or \"input features\""]},{"cell_type":"markdown","metadata":{"id":"opUEbJLPQBwc"},"source":["You saw that by using pairwise correlation, see e.g. https://en.wikipedia.org/wiki/Pearson_correlation_coefficient you could identify how correlated these variables were\n","\n","```\n","fig, ax = plt.subplots(1, 1, figsize = (8,8))\n","\n","# create a mask to white-out the upper triangle\n","mask = np.triu(np.ones_like(boston_df.corr(), dtype=bool))\n","\n","# we'll want a divergent colormap for this so our eye\n","# is not attracted to the values close to 0\n","cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","\n","sns.heatmap(boston_df.corr(), mask=mask, cmap=cmap, ax=ax)\n","\n","plt.show()\n","```"]},{"cell_type":"code","metadata":{"id":"MOq2pQqdQBwc"},"source":["fig, ax = plt.subplots(1, 1, figsize = (6,6))\n","\n","# create a mask to white-out the upper triangle\n","mask = np.triu(np.ones_like(boston_df.corr(), dtype=bool))\n","\n","# we'll want a divergent colormap for this so our eye\n","# is not attracted to the values close to 0\n","cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","\n","sns.heatmap(boston_df.corr(), mask=mask, cmap=cmap, ax=ax)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v0gFPI5jQBwc"},"source":["### 5.4.4 Linear regression on the entire Boston dataset "]},{"cell_type":"markdown","metadata":{"id":"PFQLm9NKAA5k"},"source":["\n","\n","```\n","# Create linear regression object - note that we are using all the input features\n","regr = linear_model.LinearRegression()\n","regr.fit(X, y)\n","Y_calc = regr.predict(X)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"PGb0rkrUQBwc"},"source":["# Create linear regression object - note that we are using all the input features\n","regr = linear_model.LinearRegression()\n","regr.fit(X, y)\n","Y_calc = regr.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DCrrNd-MQBwc"},"source":["Let's see what the coefficients look like ... \n","\n","```\n","print(\"Fit coefficients: \\n\", regr.coef_, \"\\nNumber of coefficients:\", len(regr.coef_))\n","```"]},{"cell_type":"code","metadata":{"id":"JwVVXevBQBwd"},"source":["print(\"Fit coefficients: \\n\", regr.coef_, \"\\nNumber of coefficients:\", len(regr.coef_))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oMOoVRCnQBwd"},"source":["We have 13 !!! That's because we are regressing respect to all **13 independent variables**!!!\n","\n","So now, $$y_{\\sf calc}= m_1x_1 +\\, m_2x_2 \\,+ \\,m_3x_3 \\,+\\,... \\,+ \\,b =\\sum_{i=1}^{13}m_i x_i + b\\;\\;\\;\\;\\; \\sf eq. 7$$\n","\n","```\n","print(\"We have 13 slopes / weights:\\n\\n\", regr.coef_)\n","print(\"\\nAnd one intercept: \", regr.intercept_)\n","```"]},{"cell_type":"code","metadata":{"id":"0MjMu6ZHQBwd"},"source":["print(\"We have 13 slopes / weights:\\n\\n\", regr.coef_)\n","print(\"\\nAnd one intercept: \", regr.intercept_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g38n0LYrASjb"},"source":["```\n","# This size should match the number of columns in X\n","if len(X[0]) == len(regr.coef_):\n","    print(\"All good! The number of coefficients matches the number of input features.\")\n","else:\n","    print(\"Hmm .. something strange is going on.\")\n","```"]},{"cell_type":"code","metadata":{"id":"n3LIvyorQBwd"},"source":["# This size should match the number of columns in X\n","if len(X[0]) == len(regr.coef_):\n","    print(\"All good! The number of coefficients matches the number of input features.\")\n","else:\n","    print(\"Hmm .. something strange is going on.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yKVDvEtMQBwd"},"source":["Let's **evaluate the error** by computing the MSE and $R^2$ metrics (see eq. 3 and 6).\n","\n","```\n","# The mean squared error\n","print('Mean squared error: %.2f' % mean_squared_error(y, Y_calc))\n","print('Coefficient of determination: %.2f' % r2_score(y, Y_calc))\n","```"]},{"cell_type":"code","metadata":{"id":"lpRwoUiGQBwd"},"source":["# The mean squared error\n","print('Mean squared error: %.2f' % mean_squared_error(y, Y_calc))\n","print('Coefficient of determination: %.2f' % r2_score(y, Y_calc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HO-kXJKUQBwd"},"source":["We can also look at how well the computed values match the true values graphically by generating a scatterplot.\n","\n","```\n","# Scatterplot\n","sns.scatterplot(x=Y_calc, y=y, color=\"cornflowerblue\", s=50)\n","plt.title(\"Linear regression - computed values on entire data set\", fontsize=16)\n","plt.xlabel(\"y$^{\\sf calc}$\")\n","plt.ylabel(\"y$^{\\sf true}$\")\n","plt.show()\n","```"]},{"cell_type":"code","metadata":{"id":"-YbuZYZ9QBwd"},"source":["# Scatterplot\n","sns.scatterplot(x=Y_calc,y=y, color=\"cornflowerblue\", s=50)\n","plt.title(\"Linear regression - computed values on entire data set\", fontsize=16)\n","plt.xlabel(\"y$^{\\sf calc}$\")\n","plt.ylabel(\"y$^{\\sf true}$\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2UydCmgrQBwd"},"source":["### 5.4.5 Let's use regression to predict the housing cost\n","\n","To see whether we can predict, we will carry out our regression only on a part, 70%, of the full data set. This part is called the **training** data. We will then test the trained model to predict the rest of the data, 30% - the **test** data. The function which fits won't see the test data until it has to predict it. \n","\n","We start by splitting out data using scikit-learn's <code> train_test_split() </code> function:"]},{"cell_type":"markdown","metadata":{"id":"RWn0QyYcQBwd"},"source":["<code> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=9) </code>"]},{"cell_type":"code","metadata":{"id":"EDf1TMuKQBwd"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amGMkkp5QBwd"},"source":["Now we check the size of <code> y_train </code> and <code> y_test </code>, the sum should be the size of y! If this works then we move on and carry out regression but we only use the training data!\n","\n","```\n","if len(y_test)+len(y_train) == len(y):\n","    \n","    print('All good, ready to C-HACK and regress!\\n')\n","    \n","    # Carry out linear regression\n","    print('Running linear regression algorithm on the training set\\n')\n","    regr = linear_model.LinearRegression()\n","    regr.fit(X_train, y_train)\n","    print('Fit coefficients and intercept:\\n\\n', regr.coef_, '\\n\\n', regr.intercept_ )\n","\n","    # Predict on the test set\n","    Y_calc_test = regr.predict(X_test)\n","```"]},{"cell_type":"code","metadata":{"id":"ExHWM4MlQBwd"},"source":["if len(y_test)+len(y_train) == len(y):\n","    \n","    print('All good, ready to C-HACK and regress!\\n')\n","    \n","    # Carry out linear regression\n","    print('Running linear regression algorithm on the training set\\n')\n","    regr = linear_model.LinearRegression()\n","    regr.fit(X_train, y_train)\n","    print('Fit coefficients and intercept:\\n\\n', regr.coef_, '\\n\\n', regr.intercept_ )\n","\n","    # Predict on the test set\n","    Y_calc_test = regr.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tff8dv_1QBwd"},"source":["Now we can plot our predicted values to see how accurate we are in predicting. We will generate a scatterplot and computing the MSE and $R^2$ metrics of error.\n","\n","```\n","sns.scatterplot(x=Y_calc_test, y=y_test, color=\"mediumvioletred\", s=50)\n","plt.title(\"Linear regression - predict test set\", fontsize=16)\n","plt.xlabel(\"y$^{\\sf calc}$\")\n","plt.ylabel(\"y$^{\\sf true}$\")\n","plt.show()\n","\n","print('Mean squared error: %.2f' % mean_squared_error(y_test, Y_calc_test))\n","print('Coefficient of determination: %.2f' % r2_score(y_test, Y_calc_test))\n","```"]},{"cell_type":"code","metadata":{"id":"TqWNcQcHQBwd"},"source":["sns.scatterplot(x=Y_calc_test, y=y_test, color=\"mediumvioletred\", s=50)\n","\n","plt.title(\"Linear regression - predict test set\", fontsize=16)\n","plt.xlabel(\"y$^{\\sf calc}$\")\n","plt.ylabel(\"y$^{\\sf true}$\")\n","plt.show()\n","\n","print('Mean squared error: %.2f' % mean_squared_error(y_test, Y_calc_test))\n","print('Coefficient of determination: %.2f' % r2_score(y_test, Y_calc_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XVxEZp3oQBwd"},"source":["#### Exercise 6\n","Play with the size of the train and test set - how does that change the error?"]},{"cell_type":"markdown","metadata":{"id":"Ri5ILlZqQBwd"},"source":["### 5.4.6 Do we need all independent variables? "]},{"cell_type":"markdown","metadata":{"id":"0sW_usSAQBwd"},"source":["We will look at prediction ability as a function of input features in the BREAKOUT ROOM, right after the break!"]},{"cell_type":"markdown","metadata":{"id":"_9cvFD6vQBwe"},"source":["### 5.4.7 Are there other ways to carry our regression?"]},{"cell_type":"markdown","metadata":{"id":"etYVqBBjQBwe"},"source":["Yes! There are many - for example Ridge Regression, LASSO, Random Forests ... you will see one of these in the BREAKOUT ROOM / one example below\n","\n","```\n","regr = linear_model.Ridge()\n","regr.fit(X_train, y_train)\n","print('Fit coefficients and intercept:\\n\\n', regr.coef_, '\\n\\n', regr.intercept_ )\n","\n","# Predict on the test set\n","Y_calc_test = regr.predict(X_test)\n","```"]},{"cell_type":"code","metadata":{"id":"cHZfKBeiQBwe"},"source":["regr = linear_model.Ridge()\n","regr.fit(X_train, y_train)\n","print('Fit coefficients and intercept:\\n\\n', regr.coef_, '\\n\\n', regr.intercept_ )\n","\n","# Predict on the test set\n","Y_calc_test = regr.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCG8yK1yAmbr"},"source":["```\n","sns.scatterplot(x=Y_calc_test, y=y_test, color=\"lightseagreen\", s=50)\n","plt.title(\"Ridge regression - predict test set\",fontsize=16)\n","plt.xlabel(\"y$^{\\sf calc}$\")\n","plt.ylabel(\"y$^{\\sf true}$\")\n","plt.show()\n","\n","print('Mean squared error: %.2f' % mean_squared_error(y_test, Y_calc_test))\n","print('Coefficient of determination: %.2f' % r2_score(y_test, Y_calc_test))\n","```"]},{"cell_type":"code","metadata":{"id":"msFDf3JVQBwe"},"source":["sns.scatterplot(x=Y_calc_test, y=y_test, color=\"lightseagreen\", s=50)\n","plt.title(\"Ridge regression - predict test set\",fontsize=16)\n","plt.xlabel(\"y$^{\\sf calc}$\")\n","plt.ylabel(\"y$^{\\sf true}$\")\n","plt.show()\n","\n","print('Mean squared error: %.2f' % mean_squared_error(y_test, Y_calc_test))\n","print('Coefficient of determination: %.2f' % r2_score(y_test, Y_calc_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zno7Go9pQBwe"},"source":["# Breakout Room\n"]},{"cell_type":"markdown","metadata":{"id":"zD4arbEoQBwe"},"source":["### Problem 1) Number and choice of input features / Bias and variance\n","\n","Load the Boston dataset and evaluate how the linear regression predictions changes as you change the **number and choice of input features**. The total number of columns in X  is 13 and each column represent a specific input feature. Estimate the MSE and compute the **variance / estimate the bias** for different training dataset slices.\n","```\n","print(X_train.shape)\n","```"]},{"cell_type":"code","metadata":{"id":"h2ICebfcQBwe"},"source":["print(X_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLn81xNxQBwe"},"source":["If you want to use the first 5 features you could proceed as following:\n","\n","```\n","X_train_five = X_train[:,0:5]\n","X_test_five = X_test[:,0:5]\n","```"]},{"cell_type":"code","metadata":{"id":"Xf9x-TLKQBwe"},"source":["X_train_five = X_train[:,0:5]\n","X_test_five = X_test[:,0:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sD92DqDIQBwe"},"source":["Check that the new variables have the shape your expect\n","\n","```\n","print(X_train_five.shape)\n","print(X_test_five.shape)\n","```"]},{"cell_type":"code","metadata":{"id":"mQx4hjiHQBwe"},"source":["print(X_train_five.shape)\n","print(X_test_five.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acc0Ec-GQBwe"},"source":["Now you can use these to train your linear regression model and repeat for different numbers or sets of input features! Note that you do not need to change the output feature! It's size is independent from the number of input features, yet recall that its length is the same as the number of values per input feature."]},{"cell_type":"markdown","metadata":{"id":"LVImG8v5QBwe"},"source":["Questions to think about while you work on this problem\n","- How many input feature variables does one need? Is there a maximum or minimum number? \n","- Are the conclusions you make on one dataset general?\n","- Could one input feature variable be better than the rest?\n","- What if values are missing for one of the input feature variables - is it still worth using it?\n","- How do the bias and variance change for each method? Try using different slices of your training dataset and compare the predictions as we did for the Cheetah"]},{"cell_type":"markdown","metadata":{"id":"-ZxX8WJiQBwe"},"source":["### Problem 2) Type of regression algorithm\n"]},{"cell_type":"markdown","metadata":{"id":"BeV8J102QBwe"},"source":["If you are already a Python expert and want to try something different, try using other types of linear regression methods on the Boston dataset: the LASSO model and the Elastic net model which are described by the \n","\n","<code > sklearn.linear_model.ElasticNet() </code> <br>\n","<code > sklearn.linear_model.Lasso() </code>\n","\n","scikit-learn functions.\n","\n","For more detail see [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) and [Lasso](  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)."]},{"cell_type":"markdown","metadata":{"id":"BlFPAeTHQBwe"},"source":["Questions to think about while you work on this problem\n","- How does the error change with each model?\n","- Which model seems to perform best?\n","- Does one model do better than the other at determining which input features are more important?\n","- How about non linear regression / what if the data does not follow a line?\n","- How do the bias and variance change for each model"]},{"cell_type":"markdown","metadata":{"id":"rfEP3u6mQBwe"},"source":["<hr style=\"border:1px solid grey\"> </hr>"]},{"cell_type":"markdown","metadata":{"id":"2CJ1j1KcQBwe"},"source":["# References"]},{"cell_type":"markdown","metadata":{"id":"cuQ-DIbZQBwe"},"source":[" * Video of a Cheetah: https://www.youtube.com/watch?v=8-9oFxYFODE\n","\n","* **Linear Regression**\n","To find out more see https://en.wikipedia.org/wiki/Simple_linear_regression\n","\n","* **scikit-learn**\n"," * Scikit-learn: https://scikit-learn.org/stable/\n"," * Linear regression in scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n"," * Metrics of error: https://scikit-learn.org/stable/modules/model_evaluation.html\n","  * The Boston dataset: https://scikit-learn.org/stable/datasets/index.html#boston-dataset\n","\n","* **Pearson correlation**\n","To find out more see https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n","\n","* **Irreducible error, bias and variance**\n"," * Great Coursera videos here: https://www.coursera.org/lecture/ml-regression/irreducible-error-and-bias-qlMrZ\n","and here: https://www.coursera.org/lecture/ml-regression/variance-and-the-bias-variance-tradeoff-ZvP40\n"]}]}